2/26/21
=======
Starting a project to make a mesh editing/finishing app in VR.  Calling it Meshwork VR for now, in homage to my old Meshwork app (and to avoid having to think up another name).

My UI is inspired by GravitySketch, which does a really good job with the UI.  But GS does not support polygon mesh creation very well, and it doesn't do UV mapping, painting, rigging, or posing at all.  So my app will focus on those things.

The plan is to make this app open-source (except for any commercial assets used), and scriptable/extensible via MiniScript.


2/27/21
=======
Much more progress on the prototype has been made.  Testing with the Spaceship model that came with Paint in 3D has been useful.  It's not a large model (880 verts, 612 tris), yet it seems to have thousands of edges, and implementing those as individual little meshes tanked my framerate.  Even using Vectrosity for those is hurting a bit.

I now realize, though, that I'm rendering too many edges, probably by a factor of 2.  I need to ensure that each edge is unique.  ...But even with that, I'm finding 5480 edges.  How can that be?!

Well, for one thing, though the mesh in Unity has 612 tris, when I look at it in code I find 1850 tris (a little more than 3X).  This appears to be because of "Seam Fixer".  Hmm.

OK, if I switch from the "fixed seams" version to the original mesh, then my MeshDisplay loads 1836 edges, which is exactly 3 per triangle.  I'd expect less than that, because of shared edges.

Ah, but the shared edges don't always share the same vertices.  Where there is a seam (or a crease), we could have multiple vertices with the same index.  Really we should sort out the overlapping vertices first, and *then* use that to identify edges.

Got that working.  Need to start thinking more about the big picture.  The major operations a user is likely to do are:

0. Sketching
1. Sculpting (modeling)
2. UV mapping
3. Painting
4. Rigging
5. Posing/animating

To avoid alienating Gravity Sketch users, I want to deemphasize 0 and 1 initially.  I've got a good demo of 3 (Painting), so I think next I want to do 2 (UV mapping).  Initially that could be simply a "UV Adjustment" tool: use it to grab a vertex (on the model or on the UV display), and drag to change its UV coordinates.

I have this UV Tweak tool sort-of working, for grabbing on the model.  The problem is, I don't know which way to shift the UV when moving the tool around; the correct answer varies depending on how the triangle hit is oriented in UV space.  Need to ponder.

For the future: I will probably need some automatic UV mapping code.  Possible references:
	https://dl.acm.org/doi/pdf/10.1145/3130800.3130845 (Autocuts)
	https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf (LSCM)
	
	
2/28/21
=======
I have my own idea for UV unwrapping.  Iterate over the six orthognal directions (+X, -X, +Y, etc.).  Select all the triangles which face more in that direction than any other, and sort them along that axis.  Now simply project each one onto that orthogonal plane, building a patch with some rectangular bounds.  If you reach a triangle that can't project into the current patch without overlapping, start a new patch; henceforth consider triangles in each of the open patches (starting with the most recent).  After doing this for all six directions, you have a bunch of patches; now arrange these into a square, and finally scale the square into the unit UV square.

Arranging rectangular boxes into a square is its own problem, but I suspect one that's not too hard.  I would sort them by area, and consider each one in two orientations 90° apart, adding them wherever we can to keep the growing collection as square as possible.

I think this approach would work great for something like a spaceship or globe; not so great for something like a character, where the fingers would produce lots of little patches.  Probably we'll need to offer several different UV unwrap methods.

I also have an idea for a cool "modifier volume" tool.  You would place a cylindrical cage in the environment, adjusting it until it encloses a set of vertices of interest.  (Or maybe it could automatically enclose the selection?)  Then you could grab either end and adjust position, rotation, and scale; these changes would be linearly interpolated along the length of the cylinder and applied to all vertices in the cage.  This provides an effective way to stretch or twist a limb, or even change scale gradually over a part.

Added a low-poly human figure "Kira" from Akishaqs (Akishaqs@outlook.com).  (Actually just selected parts of the full model, combined into one mesh in Cheetah3D.)  It shows in Unity as about 2500 verts and 2100 tris.  This causes the framerate on the Quest to drop, even though I'm still showing only about 20 draw calls.

Turning off the second directional light reduced this to 14 draw calls.  I also tried turning off the mesh display, though that is kind of important.  With those changes, the framerate seems good again.  I need to install a good FPS counter widget, and figure out exactly where the problem is.  Possibly it has to do with the Vectrosity edges and points being transparent, causing a lot of sorting and blending?  Would a cutout shader work better, performance-wise?  Need to experiment.

Also high on the to-do list: an off-hand mode menu that pops up when you hold Y, as in GS.  There are enough tools now to make that worthwhile.

Added the Graphy FPS meter.  With Kira's MeshDisplay off I get a solid FPS, but with it on I get 33 or 27 FPS (with the vertex points off or on respectively).  So that won't do.  Maybe Vectrosity is just too expensive — I might need to build a custom mesh that simply lives in 3D space, and (unlike Vectrosity) is not constantly updated whenever the camera moves.

Might look at
https://assetstore.unity.com/packages/vfx/shaders/wireframe-shader-181386
or if I want to roll my own:
https://forum.unity.com/threads/shadergraph-highlighting-edges.557005

3/01/21
=======
Got the Wireframe Shader asset.  Eager to test it out!  ...Looks like it supports both pre-baked "inside mesh" (which grows the vertex count to triangles*3), and a "dynamic shader" which does not require prebaking.  But that requires shader model 5, and does not appear to work on Quest.

Hmm, well it looks like you can bake at runtime.  This returns a new mesh with the needed extra data in uv4.  Made a new MeshDisplay class to use this.  Seems to work great!  Looks good and doesn't make the Quest break a sweat.

Had a problem for a while getting Paintable to play nice with the wireframe MeshDisplay.  It's important to make Paintable activate in Start, so it happens *after* the mesh baking that goes on in MeshDisplay.Awake.

And it's not quite true that it's a solid 72 fps.  I'm seeing occasional multi-frame stutters.  Possibly this is from the UV panels, which are displaying 2X as many edges as they really need to.  Added some code to UVMapPanel to avoid creating duplicate edges.

UI idea: instead of a close box, once we implement stretch-scaling, close a floating panel by simply squashing it down below some minimum size, at which point it should disappear with a pop.

Set up a simple survey: https://forms.gle/Ck9UB5xKaynXrRCa6

I feel I need a better intro to the project, though.  Maybe I'll try to record and cut a short video.

3/02/21
=======
Fixed a problem noticed in the video: I couldn't tweak the spaceship vertices.  Turned out to be another order-of-initialization problem: MeshModel was grabbing the pre-baked model instead of the version actually displayed by MeshDisplay.

I asked in the Paint-in-3d forum thread (https://forum.unity.com/threads/539782/) about the problem of paint leaking onto other faces, even ones facing away from the camera.  The author (Darkcoder) replied:

> You can use the P3dPaintDecal component for this with the Normal settings. If you set
> a circle shape and set Wrapping to 1 then it will look like P3dPaintSphere, but with 
> more options. 

So this is definitely something I should try.  Last night I installed Procreate on my phone, a popular painting program recommended by Emily.  It has a huge number of brushes, but most of them appear to be simply applying color through a mask texture.  I bet I can do that with P3dPaintDecal.  So it won't be hard for MeshworkVR to have a huge number of brushes, too (if I can find artists to develop these brush textures, or some open-license source for same).

Ooh, looks like there are some:
	https://opengameart.org/content/60-free-gimp-krita-brushes
Format info: https://docs.gimp.org/en/gimp-using-brushes.html
https://www.gimp.org/tutorials/Image_Pipes/
https://gitlab.gnome.org/GNOME/gimp/-/blob/master/devel-docs/gih.txt
https://gitlab.gnome.org/GNOME/gimp/-/blob/master/devel-docs/gbr.txt

I should definitely add support for GBR and GIH format brushes directly to MeshworkVR, and make these trivial for users to add to extend the app.

Apparently PhotoShop also has a .abr format, which GIMP can also read, but I haven't yet looked into how complex that is.

Those brushes are essentially sets of decals applied at the brush position.  I think there's also a concept of "texture brushes" where the brush applies a portion of a repeating texture, like https://opengameart.org/content/woodland-animals-texture-pack.

Wrote some code to parse a .gbr file, and produced my first textured brush in MVR!  Still need to add support for .gih files (animated brushes), and better integrate these brushes with the overall pipeline.  But as a tech demo, it's there.

Regarding layers, the other major tech we need, Darkcoder wrote:

> To paint multiple layers I recommend you use multiple P3dPaintableTextures. To pick 
> which texture gets painted to you can use the P3dPaintableTexture.Group setting (works 
> just like layers), where each painting component also has a Groups setting that must 
> match. If you don't want to use groups then you can just enable and disable the 
> P3dPaintableTexture components depending on which layers you wish to paint. 

For displaying them, he adds:

> As long as your mesh has no submeshes (besides the default), you can add as many 
> material 'layers' as you like, provided they're transparent. This would require a bit of 
> code to manage, but it's probably the easiest and most flexible system. 

Sounds easy enough.  So, I guess that's how we'll do that.

A Reddit user suggested some possible competition:

> Have you looked at masterpiece? It's pretty janky, but does defined if this stuff. Its 
> rigging feature is really neat.


3/04/21
=======
I've installed GIMP so that I can be certain what these brushes are supposed to look like in their native environment.  The animated brushes seem to just pick a random frame on each hit.  I think that won't be too hard to do.

We have an issue currently where when you first apply the brush, it doesn't hit right away.  Instead you have to drag a ways before it hits (if the brush has significant spacing).  I asked @Darkcoder (author of PaintIn3D) about it, and he said:

> It's possible I made a mistake and so it doesn't paint on the first frame your 
> finger/mouse goes down, when it should.
>
> I'm currently rewriting the way P3dHitScreen and the hit connection stuff works to more 
> easily allow for more advanced tools. If it still doesn't work in the next update please 
> let me know! 

...which is not too helpful.  I may dig into this more myself.

Meanwhile, I added basic animated-brush support.  Seems to work and is fun to use.


3/05/21
=======
Creating some community discussion areas:

	https://www.reddit.com/r/MeshworkVR
	https://discord.gg/s4WUwn2tjR

Also, this morning I added a basic tools panel that allows you to select a tool in each hand.  Working well.  But it really needs a tab for each mode; this will both make it less cluttered, and make it clear when you're changing modes (so we can change the way the models are rendered or whatever).

Trying to set up a Discord<-GitHub integration.  I forgot to append /github to the webhook URL (not sure why this is necessary, but apparently it is).


3/06/21
=======
Added info panels that appear over the tools when you hold B/Y.  Ultimately this is also where we will put tool options, if any are needed.  For now I'm digging this general approach: A/X is a "modifier" button (something like holding Option on desktop), and B/Y is tool help/options.

I also updated the UV Tweak tool so that the UV map display is updated.  Eventually I need to make it so you can also use this tool directly on the UV display, in addition to using it on the model.  But I can revisit that in Week 5 (UV Mapping).

Right now I need to consider what else needs to go into this before distributing a first build.  Maybe two-hand scaling?  I also considered a basic menu system, but I think we can live without that until Week 3 (UI).


3/07/21
=======
Added model scaling.  Almost time to post the first build!

First though I want to prototype one more tool: the "brush" tool as Emily described, where the effective diameter varies with how far you are from the surface.  I realized this is basically the same as the existing cone tool, except the cone is shorter and inverted, and we don't let distance affect opacity (we'll control that soley with the trigger).


3/08/21
=======
Got some great feedback from users on the first build.  Some take-aways:
	- brush beam/cone needs to be more clear, especially when resizing
	- probably need to adjust the cone size/shape limits
	- grab ball needs visual/haptic feedback
	- need a way to grab empty space to transform the entire scene
	- Brush panel is distracting when showing an animated brush

I'm not sure yet what, if anything, to do about that last one; but the others are all pretty straightforward.  With regard to two-hand grabbing: right now we use the second hand for scale, but still take position and rotation entirely from the primary grab.  I think it'd be better to calculate the best 2-hand transform fit for position and rotation, too.

Reworked the two-hand manipulator code substantially.  It still has a couple of issues: sometimes the object gets "stuck" to one hand until you squeeze the grip again, and a couple times, the whole scene has winked out of existence (with complaints in the logs about a negative scale).  But the basic functionality is pretty much what we want.


3/09/21
=======
Started work on supporting layers today.  Multiple transparent materials on one model does appear to work (though I'll need to change how the wireframe is applied, so that it applies to the last material rather than the first -- if we want it on at all during painting).

Note that it seems like PaintIn3D is using a render texture, not directly editing the original material texture.  There are methods to get a copy of this as a Texture2D, but that's an expensive operation.

So instead, I've set up a RawImage to directly use the material on the model, which is using the render texture being updated by the paint.  This works fine on the second layer; the first layer is looking a little haywire.  This might be because of the wireframe shader, so if we decide to remove wireframe while painting, that would become a non-issue.

Also, I had alpha on the color panel incorrectly hooked up to the blue channel.  Now it's hooked up to alpha.  By turning alpha down, you can really reduce the opacity of the paint you apply.  This is all working exactly as it should... but (combined with layering) makes me realize that we need an eraser mode or tool.  I'm tempted to just pop all the paint tools into erase mode when you hold the A/X button.  But that does mean we lose that button for color selection.  Hmm.  Maybe that's OK though, as you could just dock the color panel to your off arm.

I should note that there are a whole slew of other blend modes that may be useful.  I should think about which of these to expose, and how.  They are nicely illustrated in the P3D documentation.  For now, I just switch to Subtractive mode (alpha only) when you press A/X to erase, and that seems to be working fine.


3/10/21
=======
Just realized that our two-hand manipulation still isn't quite right, with regard to rotation: we should be using Transform.RotateAround to ensure that we're rotating the object around the point between our hands, rather than around its own axis.


3/11/21
=======
A bit more work on layers today.  I have them showing/hiding nicely.  I started work on directing the paint to the selected layer, by enabling/disabling P3dPaintableTexture components on the model as suggested by @Darkcoder.  But this did not work for me: when I switch layers, previous painting suddenly appeared on the newly enabled texture.  I've posted to https://forum.unity.com/threads/539782/ about it.


3/13/21
=======
@Darkcoder has kindly prepared a new build of his PaintIn3D plugin, with an example showing how to do layers (a bit different from what he advised before).

OK, his example is overcomplicated, but it boils down to this: rather than directly  enabling/disabling the P3dPaintableTextures, set their Group to -1 to turn them off, and to 0 (matching the group on the paint Paint Decal component) to turn them on.

But I see the new version of P3dPaintDecal also has targetModel and targetTexture properties, which allow me to restrict painting to a specific target.  That might work too.

However, other changes in this build are troubling me. P3dHitBetween previously derived from P3dConnectablePoints; I was using the HitSpacing component of that to control the hit spacing as for the brush size and type. (Though this didn't work perfectly as noted in the forum — often I would not get a hit on the initial pointer-down.)

But in the new build, P3dHitBetween derives directly from MonoBehaviour, and has no HitSpacing component. How do we control the hit spacing now? 
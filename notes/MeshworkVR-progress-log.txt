2/26/21
=======
Starting a project to make a mesh editing/finishing app in VR.  Calling it Meshwork VR for now, in homage to my old Meshwork app (and to avoid having to think up another name).

My UI is inspired by GravitySketch, which does a really good job with the UI.  But GS does not support polygon mesh creation very well, and it doesn't do UV mapping, painting, rigging, or posing at all.  So my app will focus on those things.

The plan is to make this app open-source (except for any commercial assets used), and scriptable/extensible via MiniScript.


2/27/21
=======
Much more progress on the prototype has been made.  Testing with the Spaceship model that came with Paint in 3D has been useful.  It's not a large model (880 verts, 612 tris), yet it seems to have thousands of edges, and implementing those as individual little meshes tanked my framerate.  Even using Vectrosity for those is hurting a bit.

I now realize, though, that I'm rendering too many edges, probably by a factor of 2.  I need to ensure that each edge is unique.  ...But even with that, I'm finding 5480 edges.  How can that be?!

Well, for one thing, though the mesh in Unity has 612 tris, when I look at it in code I find 1850 tris (a little more than 3X).  This appears to be because of "Seam Fixer".  Hmm.

OK, if I switch from the "fixed seams" version to the original mesh, then my MeshDisplay loads 1836 edges, which is exactly 3 per triangle.  I'd expect less than that, because of shared edges.

Ah, but the shared edges don't always share the same vertices.  Where there is a seam (or a crease), we could have multiple vertices with the same index.  Really we should sort out the overlapping vertices first, and *then* use that to identify edges.

Got that working.  Need to start thinking more about the big picture.  The major operations a user is likely to do are:

0. Sketching
1. Sculpting (modeling)
2. UV mapping
3. Painting
4. Rigging
5. Posing/animating

To avoid alienating Gravity Sketch users, I want to deemphasize 0 and 1 initially.  I've got a good demo of 3 (Painting), so I think next I want to do 2 (UV mapping).  Initially that could be simply a "UV Adjustment" tool: use it to grab a vertex (on the model or on the UV display), and drag to change its UV coordinates.

I have this UV Tweak tool sort-of working, for grabbing on the model.  The problem is, I don't know which way to shift the UV when moving the tool around; the correct answer varies depending on how the triangle hit is oriented in UV space.  Need to ponder.

For the future: I will probably need some automatic UV mapping code.  Possible references:
	https://dl.acm.org/doi/pdf/10.1145/3130800.3130845 (Autocuts)
	https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf (LSCM)
	
	
2/28/21
=======
I have my own idea for UV unwrapping.  Iterate over the six orthognal directions (+X, -X, +Y, etc.).  Select all the triangles which face more in that direction than any other, and sort them along that axis.  Now simply project each one onto that orthogonal plane, building a patch with some rectangular bounds.  If you reach a triangle that can't project into the current patch without overlapping, start a new patch; henceforth consider triangles in each of the open patches (starting with the most recent).  After doing this for all six directions, you have a bunch of patches; now arrange these into a square, and finally scale the square into the unit UV square.

Arranging rectangular boxes into a square is its own problem, but I suspect one that's not too hard.  I would sort them by area, and consider each one in two orientations 90° apart, adding them wherever we can to keep the growing collection as square as possible.

I think this approach would work great for something like a spaceship or globe; not so great for something like a character, where the fingers would produce lots of little patches.  Probably we'll need to offer several different UV unwrap methods.

I also have an idea for a cool "modifier volume" tool.  You would place a cylindrical cage in the environment, adjusting it until it encloses a set of vertices of interest.  (Or maybe it could automatically enclose the selection?)  Then you could grab either end and adjust position, rotation, and scale; these changes would be linearly interpolated along the length of the cylinder and applied to all vertices in the cage.  This provides an effective way to stretch or twist a limb, or even change scale gradually over a part.

Added a low-poly human figure "Kira" from Akishaqs (Akishaqs@outlook.com).  (Actually just selected parts of the full model, combined into one mesh in Cheetah3D.)  It shows in Unity as about 2500 verts and 2100 tris.  This causes the framerate on the Quest to drop, even though I'm still showing only about 20 draw calls.

Turning off the second directional light reduced this to 14 draw calls.  I also tried turning off the mesh display, though that is kind of important.  With those changes, the framerate seems good again.  I need to install a good FPS counter widget, and figure out exactly where the problem is.  Possibly it has to do with the Vectrosity edges and points being transparent, causing a lot of sorting and blending?  Would a cutout shader work better, performance-wise?  Need to experiment.

Also high on the to-do list: an off-hand mode menu that pops up when you hold Y, as in GS.  There are enough tools now to make that worthwhile.

Added the Graphy FPS meter.  With Kira's MeshDisplay off I get a solid FPS, but with it on I get 33 or 27 FPS (with the vertex points off or on respectively).  So that won't do.  Maybe Vectrosity is just too expensive — I might need to build a custom mesh that simply lives in 3D space, and (unlike Vectrosity) is not constantly updated whenever the camera moves.

Might look at
https://assetstore.unity.com/packages/vfx/shaders/wireframe-shader-181386
or if I want to roll my own:
https://forum.unity.com/threads/shadergraph-highlighting-edges.557005

3/01/21
=======
Got the Wireframe Shader asset.  Eager to test it out!  ...Looks like it supports both pre-baked "inside mesh" (which grows the vertex count to triangles*3), and a "dynamic shader" which does not require prebaking.  But that requires shader model 5, and does not appear to work on Quest.

Hmm, well it looks like you can bake at runtime.  This returns a new mesh with the needed extra data in uv4.  Made a new MeshDisplay class to use this.  Seems to work great!  Looks good and doesn't make the Quest break a sweat.

Had a problem for a while getting Paintable to play nice with the wireframe MeshDisplay.  It's important to make Paintable activate in Start, so it happens *after* the mesh baking that goes on in MeshDisplay.Awake.

And it's not quite true that it's a solid 72 fps.  I'm seeing occasional multi-frame stutters.  Possibly this is from the UV panels, which are displaying 2X as many edges as they really need to.  Added some code to UVMapPanel to avoid creating duplicate edges.

UI idea: instead of a close box, once we implement stretch-scaling, close a floating panel by simply squashing it down below some minimum size, at which point it should disappear with a pop.

Set up a simple survey: https://forms.gle/Ck9UB5xKaynXrRCa6

I feel I need a better intro to the project, though.  Maybe I'll try to record and cut a short video.

3/02/21
=======
Fixed a problem noticed in the video: I couldn't tweak the spaceship vertices.  Turned out to be another order-of-initialization problem: MeshModel was grabbing the pre-baked model instead of the version actually displayed by MeshDisplay.

I asked in the Paint-in-3d forum thread (https://forum.unity.com/threads/539782/) about the problem of paint leaking onto other faces, even ones facing away from the camera.  The author (Darkcoder) replied:

> You can use the P3dPaintDecal component for this with the Normal settings. If you set
> a circle shape and set Wrapping to 1 then it will look like P3dPaintSphere, but with 
> more options. 

So this is definitely something I should try.  Last night I installed Procreate on my phone, a popular painting program recommended by Emily.  It has a huge number of brushes, but most of them appear to be simply applying color through a mask texture.  I bet I can do that with P3dPaintDecal.  So it won't be hard for MeshworkVR to have a huge number of brushes, too (if I can find artists to develop these brush textures, or some open-license source for same).

Ooh, looks like there are some:
	https://opengameart.org/content/60-free-gimp-krita-brushes
Format info: https://docs.gimp.org/en/gimp-using-brushes.html
https://www.gimp.org/tutorials/Image_Pipes/
https://gitlab.gnome.org/GNOME/gimp/-/blob/master/devel-docs/gih.txt
https://gitlab.gnome.org/GNOME/gimp/-/blob/master/devel-docs/gbr.txt

I should definitely add support for GBR and GIH format brushes directly to MeshworkVR, and make these trivial for users to add to extend the app.

Apparently PhotoShop also has a .abr format, which GIMP can also read, but I haven't yet looked into how complex that is.

Those brushes are essentially sets of decals applied at the brush position.  I think there's also a concept of "texture brushes" where the brush applies a portion of a repeating texture, like https://opengameart.org/content/woodland-animals-texture-pack.

Wrote some code to parse a .gbr file, and produced my first textured brush in MVR!  Still need to add support for .gih files (animated brushes), and better integrate these brushes with the overall pipeline.  But as a tech demo, it's there.

Regarding layers, the other major tech we need, Darkcoder wrote:

> To paint multiple layers I recommend you use multiple P3dPaintableTextures. To pick 
> which texture gets painted to you can use the P3dPaintableTexture.Group setting (works 
> just like layers), where each painting component also has a Groups setting that must 
> match. If you don't want to use groups then you can just enable and disable the 
> P3dPaintableTexture components depending on which layers you wish to paint. 

For displaying them, he adds:

> As long as your mesh has no submeshes (besides the default), you can add as many 
> material 'layers' as you like, provided they're transparent. This would require a bit of 
> code to manage, but it's probably the easiest and most flexible system. 

Sounds easy enough.  So, I guess that's how we'll do that.

A Reddit user suggested some possible competition:

> Have you looked at masterpiece? It's pretty janky, but does defined if this stuff. Its 
> rigging feature is really neat.


3/04/21
=======
I've installed GIMP so that I can be certain what these brushes are supposed to look like in their native environment.  The animated brushes seem to just pick a random frame on each hit.  I think that won't be too hard to do.

We have an issue currently where when you first apply the brush, it doesn't hit right away.  Instead you have to drag a ways before it hits (if the brush has significant spacing).  I asked @Darkcoder (author of PaintIn3D) about it, and he said:

> It's possible I made a mistake and so it doesn't paint on the first frame your 
> finger/mouse goes down, when it should.
>
> I'm currently rewriting the way P3dHitScreen and the hit connection stuff works to more 
> easily allow for more advanced tools. If it still doesn't work in the next update please 
> let me know! 

...which is not too helpful.  I may dig into this more myself.

Meanwhile, I added basic animated-brush support.  Seems to work and is fun to use.


3/05/21
=======
Creating some community discussion areas:

	https://www.reddit.com/r/MeshworkVR
	https://discord.gg/s4WUwn2tjR

Also, this morning I added a basic tools panel that allows you to select a tool in each hand.  Working well.  But it really needs a tab for each mode; this will both make it less cluttered, and make it clear when you're changing modes (so we can change the way the models are rendered or whatever).

Trying to set up a Discord<-GitHub integration.  I forgot to append /github to the webhook URL (not sure why this is necessary, but apparently it is).


3/06/21
=======
Added info panels that appear over the tools when you hold B/Y.  Ultimately this is also where we will put tool options, if any are needed.  For now I'm digging this general approach: A/X is a "modifier" button (something like holding Option on desktop), and B/Y is tool help/options.

I also updated the UV Tweak tool so that the UV map display is updated.  Eventually I need to make it so you can also use this tool directly on the UV display, in addition to using it on the model.  But I can revisit that in Week 5 (UV Mapping).

Right now I need to consider what else needs to go into this before distributing a first build.  Maybe two-hand scaling?  I also considered a basic menu system, but I think we can live without that until Week 3 (UI).


3/07/21
=======
Added model scaling.  Almost time to post the first build!

First though I want to prototype one more tool: the "brush" tool as Emily described, where the effective diameter varies with how far you are from the surface.  I realized this is basically the same as the existing cone tool, except the cone is shorter and inverted, and we don't let distance affect opacity (we'll control that soley with the trigger).


3/08/21
=======
Got some great feedback from users on the first build.  Some take-aways:
	- brush beam/cone needs to be more clear, especially when resizing
	- probably need to adjust the cone size/shape limits
	- grab ball needs visual/haptic feedback
	- need a way to grab empty space to transform the entire scene
	- Brush panel is distracting when showing an animated brush

I'm not sure yet what, if anything, to do about that last one; but the others are all pretty straightforward.  With regard to two-hand grabbing: right now we use the second hand for scale, but still take position and rotation entirely from the primary grab.  I think it'd be better to calculate the best 2-hand transform fit for position and rotation, too.

Reworked the two-hand manipulator code substantially.  It still has a couple of issues: sometimes the object gets "stuck" to one hand until you squeeze the grip again, and a couple times, the whole scene has winked out of existence (with complaints in the logs about a negative scale).  But the basic functionality is pretty much what we want.


3/09/21
=======
Started work on supporting layers today.  Multiple transparent materials on one model does appear to work (though I'll need to change how the wireframe is applied, so that it applies to the last material rather than the first -- if we want it on at all during painting).

Note that it seems like PaintIn3D is using a render texture, not directly editing the original material texture.  There are methods to get a copy of this as a Texture2D, but that's an expensive operation.

So instead, I've set up a RawImage to directly use the material on the model, which is using the render texture being updated by the paint.  This works fine on the second layer; the first layer is looking a little haywire.  This might be because of the wireframe shader, so if we decide to remove wireframe while painting, that would become a non-issue.

Also, I had alpha on the color panel incorrectly hooked up to the blue channel.  Now it's hooked up to alpha.  By turning alpha down, you can really reduce the opacity of the paint you apply.  This is all working exactly as it should... but (combined with layering) makes me realize that we need an eraser mode or tool.  I'm tempted to just pop all the paint tools into erase mode when you hold the A/X button.  But that does mean we lose that button for color selection.  Hmm.  Maybe that's OK though, as you could just dock the color panel to your off arm.

I should note that there are a whole slew of other blend modes that may be useful.  I should think about which of these to expose, and how.  They are nicely illustrated in the P3D documentation.  For now, I just switch to Subtractive mode (alpha only) when you press A/X to erase, and that seems to be working fine.


3/10/21
=======
Just realized that our two-hand manipulation still isn't quite right, with regard to rotation: we should be using Transform.RotateAround to ensure that we're rotating the object around the point between our hands, rather than around its own axis.


3/11/21
=======
A bit more work on layers today.  I have them showing/hiding nicely.  I started work on directing the paint to the selected layer, by enabling/disabling P3dPaintableTexture components on the model as suggested by @Darkcoder.  But this did not work for me: when I switch layers, previous painting suddenly appeared on the newly enabled texture.  I've posted to https://forum.unity.com/threads/539782/ about it.


3/13/21
=======
@Darkcoder has kindly prepared a new build of his PaintIn3D plugin, with an example showing how to do layers (a bit different from what he advised before).

OK, his example is overcomplicated, but it boils down to this: rather than directly  enabling/disabling the P3dPaintableTextures, set their Group to -1 to turn them off, and to 0 (matching the group on the paint Paint Decal component) to turn them on.

But I see the new version of P3dPaintDecal also has targetModel and targetTexture properties, which allow me to restrict painting to a specific target.  That might work too.  (Later edit: yes, this seems to work better, so I'm using that now.)

However, other changes in this build are troubling me. P3dHitBetween previously derived from P3dConnectablePoints; I was using the HitSpacing component of that to control the hit spacing as for the brush size and type. (Though this didn't work perfectly as noted in the forum — often I would not get a hit on the initial pointer-down.)

But in the new build, P3dHitBetween derives directly from MonoBehaviour, and has no HitSpacing component. How do we control the hit spacing now?  ...Turns out we now get to it via a new .Connector intermediary.


3/14/21
=======
Still struggling to get layers working reliably.  Much progress has been made, but everything with this P3d asset seems much more complicated than I would expect.

Had a long time just now when nothing would draw on the first layer, though it would draw on subsequent layers just fine.  This even after turning off almost all the code.  It turned out to be: I had changed MeshDisplay to make all of its work to set up a wireframe display optional.  But something about that work makes P3d function; without it, I can't paint on the material at all.  Probably I need to clone the material or something.

So for now, I'll just leave the wireframe on.

Next issue: I need to keep PaintableTextures on the painting quad (on the UV map panel) in sync with those on the model.  This is important to making sure you can paint on the UV map as well as on the model itself.

And, it turns out, turning the wireframe back on means that this layer no longer displays on the UV map or Layers panel.  Argh.

OK, the UV map and Layers are showing a big blue panel because of the wireframe shader.  I can see that the solid color of the RawImage matches the color of the wireframe, and this draws even when I turn the wireframe width all the way down to 0.  Of course any changes here also affect it on the model.

So I need to find a way to use a different material, that nonetheless shows the same texture.  Or no material, and just show the texture using RawTexture.texture.  ...OK, that works, as long as you have the timing right: it needs to show the temp buffer texture created by the P3d system, not the original texture.

Now the UV map is showing the right texture, but I can't paint on it.  Experimenting shows that this is caused by the new code in PaintLayersPanel that assigns a TargetTexture to the P3dPaintDecal.  But I'm surprised that doesn't work, since the Quad and the model should be using the *same* PaintableTexture.

Ah.  But I think I see: P3dPaintableManager:65 says, when we have a targetTexture, submit the painting command to its Paintable.  And that Paintable would refer to the wrong component in this case (i.e. the one on the model, not the UV Map quad).  Dang.

Well.  You can also assign a targetModel (P3dModel), and if set, that will be used instead.  I haven't been using this component, but maybe I should?  Ah yes: the docs actually mention this:

> Atlas Painting
>
> This shows you how to paint multiple separate objects that share the same texture. This 
> is done by setting up one paintable object normally, and then dragging and dropping the 
> other objects into the P3dPaintable component's Advanced / OtherRenderers list. You must 
> then add the P3dModel component to the other objects, and drag and drop the first object > into the P3dModel.Paintable setting.

I'll attempt to hook it up just as described.  It displays fine, but it's still not painting, no doubt still because of the targetTexture.  If using that, I think I need to use targetModel too.  But P3dHitBetween doesn't offer any way to get the object that was hit, or to hook into the processing thereof.

But I have verified (through a test hack) that correctly setting the targetModel makes this work.  So I'll just have to cast my own ray in PaintSprayTool.  (The alternative would be to hack P3dHitBetween, but I'm not prepared to do that just yet.)

OK, this is working.  But to make it work, I had to do an unfortunate hack: it's hitting the canvas collider, not the quad collider.  So I'm having to use GetComponentinChildren to find the P3dModel.  This appears to work, but seems fragile.  However I can't easily resolve it with layers, since in this case the grabbable and paintable objects are separate, but in other cases (like the 3D model) they are the same GameObject, and a GameObject can't be in two layers.

But I suppose I could have a special layer for paintable-but-not-grababble, and then have the grab raycast ignore those, while the paint tools cast for that as well as normal (paintable/grabbable) objects.  I'm leaving this as a to-do item for the future.

So!  We *finally* have the basic functionality of layers and painting on both the model and the atlas working.  One thing I need to fix before doing this week's build: right now erasing on the bottommost layer looks wrong, because it's clearing the alpha channel, but it's not a transparent shader.  I think we need to either switch that layer to allow transparency, or erase to white (on this layer only).

3/15/21
=======
Trying to wrap up the week-o-painting, one day late.  Hooking up the spaceship to its panels.  Doing this manually for now; later, when we have UI, obviously we'll need to do this from code.  The required steps are:

1. On the model's PaintableTexture component, drag the Painting Quad (of the UVMap Canvas) into the Other Renderers list.
2. Conversely, on the Painting Quad, drag the model into the Model component's Paintable slot.
3. On the Paint Layers Panel, hook up the Model.
4. On the same panel, hook up the onMaterialSelected event to call UVMapPanel.NoteMaterialSelected.

I've run into a snag wherein the second Paint Layers Canvas, set up for the Ship, is not usable as UI with the XR Interaction rays.  It works fine in the editor with the mouse, but with the XR interactors the controls do not highlight or react.   ...Ah.  The canvas did not have its worldCamera set.  I've added something to CanvasColliderAdjuster to automatically assign Camera.main in Start, if no other value is set.

4/06/21
=======
Initial painting is done.  Working now on UI and file I/O.

My original plan was to support OBJ, but a user has (quite sensibly) requested support for GLTF/GLB.  The Khronos Group has a decent Unity GLTF library (https://github.com/KhronosGroup/UnityGLTF).

As for OBJ, I thought I had some code lying around for reading it, but it appears this is not true.  I have custom code for writing OBJ, but for reading it, I've been using a commercial asset.  However I found a free, liberally-licensed library for it in this thread: https://forum.unity.com/threads/free-runtime-obj-loader.365884/

That code is blocking, and it mishandles negative indices, but with a bit of polish it looks like it could be made to work.  And it is a *much* smaller library and easier build process than UnityGLTF.

So I'm going to start today with OBJ import (for reference objects), and then try to add GLTF perhaps tomorrow.

4/18/21
=======
OBJ and GLTF import and export are both working.  Moving on to modeling now.

To that end, I've changed the VertexTweak tool into a more general MeshTweak tool.  It has three sub-modes: vertex, edge, and face.  Vertex works as before; deferring edge for now and working on Face mode.

The idea here is that you will be able to grab a face (usually a triangle or quad, which is really a pair of coplanar triangles) and drag it around or rotate it with the tool.  An initial implementation has that partly working: you can grab and drag a triangle, but it doesn't get the other half of the quad (if any) and does not yet support rotation.

I think to do this properly, I need to gather the positions of the affected vertices at the start of the drag; store those as local to the tool; and then on drag, convert those local positions back into (new) positions local to the mesh, and apply them that way.

Implemented and working, though the code that figures out what other triangle should be considered part of the same face is a little dicey.  It works fine once, but as you edit the model the coplanarity is quickly lost, and then it doesn't work anymore.  Possibly we need to work out all the quads (in the same way the wireframe shader does?) when the model is first loaded, and then maintain that information while editing even if the triangles in a quad are no longer truly coplanar.


4/21/21
=======
Starting on selection support.  This is trickier than it appears; I need a way to draw a highlight on certain faces, edges, or vertices.

Vertices will probably just use a little box (perhaps drawn with DrawInstanced?) for each vertex, so that's straightforward.  But edges and vertices are trickier.  We want this highlight to appear between the normal texture of the object, and the wireframe.

Right now that's all drawn at once, using the standard Amazing Wireframe shader.  I could separate it out, and use multiple materials to draw (1) the various texture layers, (2) the highlight, and (3) the wireframe (configured to clip wherever the wireframe isn't).  Of course that's more draw passes.  But hopefully you won't have dozens of editable objects in the scene at once, so maybe it's fine.

How to actually draw the highlights?  Let's take face and edge highlighting separately (and ignore vertices for now since they're easy).

To highlight faces, I might be able to use vertex colors, IF we are never sharing vertices between faces.  It's possible that's already true, due to requirements of PaintIn3D or Amazing Wireframe (though I doubt the latter).  If so we may as well take advantage of this for the highlighting too.

If that's not true, then I can't use vertex colors.  Instead I would need a shader that uses the triangle index, accessible in the fragment function as a SV_PrimitiveID parameter (after the usual v2f parameter).  It would then need to look up that triangle ID by sampling a texture that has one row (or column) for each triangle in the mesh.

(Update: I just found Amazing Wireframe docs saying that when you bake a mesh for use with this shader, every triangle has its own unique vertices.  So I guess vertex colors should work fine.)

For highlighting edges, I would need to make a custom wireframe shader (fortunately Amazing Wireframe is organized as an include file that is easy to incorporate into custom shaders).  I can get the vertex ID in the vertex function as a second SV_VertexID parameter.  However I'm not sure how that helps, since the highlighting of an edge is a function of TWO vertices.  The vert function has one, and the frag function has at best a mixture of three.  Hmm.

I wonder if there's some clever way to make use of vertex colors such that they only cross some threshold when _both_ vertices at either end of an edge are selected.


6/13/21
=======
Yikes.  A bit of a hiatus just happened due to other commitments.  I've got another one coming up the week after that, but otherwise I should be good to push forward on this.

Reviewing the current status: highlighting and manipulating faces (triangles, for now) is working pretty well.  I think the next step is to support edge and vertex mode.  The previous entry contains thoughts about highlighting edges.

The tricky case here is when two of the edges in a triangle are selected, but not the third.  In this case all three vertices are part of a selection, but only certain pairs should be drawn with a highlighted edge.  How to tell when you are part of such a pair?

After some experimentation, I have settled on this solution: we set the uv2 coordinates of the vertices in each triangle in a very specific way: (0,0), (1,0), and (0.5,1).  Then the shader does a bit of math from the uv2 coordinates to figure out which side of the triangle it is on.  Finally, we store a vertex color for each triangle where the red, green, and blue components indicate whether each corresponding edge is selected, and use that to select an appropriate color.  A little complex, but it looks great and performs well.


6/30/21
=======
Started integrating the edge coloring technique into the tools.  I've started with the Selection tool because it is much simpler than the tweak tool.  Ultimately, I think these need to be unified -- perhaps the tweak tool can derive from the selection tool?

Current status: using the selection tool, and manually replacing the material on the test cube with MeshworkEdgeMat (which uses the custom edge shader), you can now select edges and they are properly highlighted.  The next step will be to make MeshDisplay automatically change the material based on the current mode, and then to integrate the new functionality into TweakTool (perhaps refactoring to share code with SelectionTool).
